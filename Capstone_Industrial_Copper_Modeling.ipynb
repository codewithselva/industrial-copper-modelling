{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/codewithselva/industrial-copper-modelling/blob/main/Capstone_Industrial_Copper_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "The copper industry deals with less complex data related to sales and pricing. However, this data may suffer from issues such as skewness and noisy data, which can affect the accuracy of manual predictions. Dealing with these challenges manually can be time-consuming and may not result in optimal pricing decisions. A machine learning regression model can address these issues by utilizing advanced techniques such as data normalization, feature scaling, and outlier detection, and leveraging algorithms that are robust to skewed and noisy data. \n",
    "\n",
    "Another area where the copper industry faces challenges is in capturing the leads. A lead classification model is a system for evaluating and classifying leads based on how likely they are to become a customer . You can use the STATUS variable with WON being considered as Success and LOST being considered as Failure and remove data points other than WON, LOST STATUS values.\n",
    "\n",
    "\n",
    "The solution must include the following steps:\n",
    "1. Exploring skewness and outliers in the dataset.\n",
    "2. Transform the data into a suitable format and perform any necessary cleaning and pre-processing steps.\n",
    "3. ML Regression model which predicts continuous variable ‘Selling_Price’.\n",
    "4. ML Classification model which predicts Status: WON or LOST.\n",
    "5. Creating a streamlit page where you can insert each column value and you will get the Selling_Price predicted value or Status(Won/Lost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzMt5JYAegNT"
   },
   "source": [
    "# About the Data:\n",
    "1. `id`: This column likely serves as a unique identifier for each transaction or item, which can be useful for tracking and record-keeping.\n",
    "2. `item_date`: This column represents the date when each transaction or item was recorded or occurred. It's important for tracking the timing of business activities.\n",
    "3. `quantity tons`: This column indicates the quantity of the item in tons, which is essential for inventory management and understanding the volume of products sold or produced.\n",
    "4. `customer`: The \"customer\" column refers to the name or identifier of the customer who either purchased or ordered the items. It's crucial for maintaining customer relationships and tracking sales.\n",
    "5. `country`: The \"country\" column specifies the country associated with each customer. This information can be useful for understanding the geographic distribution of customers and may have implications for logistics and international sales.\n",
    "6. `status`: The \"status\" column likely describes the current status of the transaction or item. This information can be used to track the progress of orders or transactions, such as \"Draft\" or \"Won.\"\n",
    "7. `item type`: This column categorizes the type or category of the items being sold or produced. Understanding item types is essential for inventory categorization and business reporting.\n",
    "8. `application`: The \"application\" column defines the specific use or application of the items. This information can help tailor marketing and product development efforts.\n",
    "9. `thickness`: The \"thickness\" column provides details about the thickness of the items. It's critical when dealing with materials where thickness is a significant factor, such as metals or construction materials.\n",
    "10. `width`: The \"width\" column specifies the width of the items. It's important for understanding the size and dimensions of the products.\n",
    "11. `material_ref`: This column appears to be a reference or identifier for the material used in the items. It's essential for tracking the source or composition of the products.\n",
    "12. `product_ref`: The \"product_ref\" column seems to be a reference or identifier for the specific product. This information is useful for identifying and cataloging products in a standardized way.\n",
    "13. `delivery date`: This column records the expected or actual delivery date for each item or transaction. It's crucial for managing logistics and ensuring timely delivery to customers.\n",
    "14. `selling_price`: The \"selling_price\" column represents the price at which the items are sold. This is a critical factor for revenue generation and profitability analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amxsmVAQhgd2"
   },
   "source": [
    "# Approach: \n",
    "1. Data Understanding: Identify the types of variables (continuous, categorical) and their distributions. Some rubbish values are present in ‘Material_Reference’ which starts with ‘00000’ value which should be converted into null. Treat reference columns as categorical variables. INDEX may not be useful.\n",
    "2. Data Preprocessing:\n",
    "Handle missing values with mean/median/mode.\n",
    "Treat Outliers using IQR or Isolation Forest from sklearn library.\n",
    "Identify Skewness in the dataset and treat skewness with appropriate data transformations, such as log transformation(which is best suited to transform target variable-train, predict and then reverse transform it back to original scale eg:dollars), boxcox transformation, or other techniques, to handle high skewness in continuous variables.\n",
    "Encode categorical variables using suitable techniques, such as one-hot encoding, label encoding, or ordinal encoding, based on their nature and relationship with the target variable.\n",
    "3. EDA: Try visualizing outliers and skewness(before and after treating skewness) using Seaborn’s boxplot, distplot, violinplot.\n",
    "4. Feature Engineering: Engineer new features if applicable, such as aggregating or transforming existing features to create more informative representations of the data. And drop highly correlated columns using SNS HEATMAP.\n",
    "5. Model Building and Evaluation:\n",
    "Split the dataset into training and testing/validation sets.\n",
    "Train and evaluate different classification models, such as ExtraTreesClassifier, XGBClassifier, or Logistic Regression, using appropriate evaluation metrics such as accuracy, precision, recall, F1 score, and AUC curve.\n",
    "Optimize model hyperparameters using techniques such as cross-validation and grid search to find the best-performing model.\n",
    "Interpret the model results and assess its performance based on the defined problem statement.\n",
    "Same steps for Regression modelling.(note: dataset contains more noise and linearity between independent variables so itll perform well only with tree based models)\n",
    "6. Model GUI: Using streamlit module, create interactive page with\n",
    "   (1) task input( Regression or Classification) and\n",
    "   (2) create an input field where you can enter each column value except ‘Selling_Price’ for regression model and  except ‘Status’ for classification model.\n",
    "   (3) perform the same feature engineering, scaling factors, log/any transformation steps which you used for training ml model and predict this new data from streamlit and display the output.\n",
    "7. Tips: Use pickle module to dump and load models such as encoder(onehot/ label/ str.cat.codes /etc), scaling models(standard scaler), ML models. First fit and then transform in separate line and use transform only for unseen data\n",
    "Eg: scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test_new) #unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KLBx9hHCPUzl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malar\\documents\\github_codewithselva\\data-visualization-plotly\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install openpyxl\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn \n",
    "!pip install scipy \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNMR5SllTMKL"
   },
   "outputs": [],
   "source": [
    "#run this command to mount the google drive while using colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm9t-raWZc_6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMTskHScZ230"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file and load it into a Pandas DataFrame\n",
    "excel_file_path = 'copper_data_set.csv'\n",
    "\n",
    "df = pd.read_csv(excel_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1w2BWe_Xfvg"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DY3i6Unlib11"
   },
   "outputs": [],
   "source": [
    "# Total number of records in the data set\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSR_ZU1TdXry"
   },
   "outputs": [],
   "source": [
    "# Display the info of the DataFrame\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgP1MKgziH8S"
   },
   "source": [
    "**Inference:**\n",
    "1. Total number of records: 181673\n",
    "2. item_date field is in float64 Dtype which needs to be converted into date Dtype\n",
    "3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhP5OqAMoh6I"
   },
   "outputs": [],
   "source": [
    "# Create a copy to avoid modifying the original DataFrame\n",
    "cleaned_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phs5lHZlolgD"
   },
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_unique = cleaned_df['status'].unique()\n",
    "status_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df[(cleaned_df['status'] == 'Won') | (cleaned_df['status'] == 'Lost')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rT4SHKTQyIVp"
   },
   "outputs": [],
   "source": [
    "# Assuming 'your_column' is the column you're working with\n",
    "cleaned_df['quantity tons'] = pd.to_numeric(cleaned_df['quantity tons'], errors='coerce')\n",
    "cleaned_df.sort_values(by='quantity tons', ascending=False, inplace=True)\n",
    "cleaned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvlj0AckPUzr"
   },
   "outputs": [],
   "source": [
    "cleaned_df['item_date'] = pd.to_datetime(cleaned_df['item_date'], format='%Y%m%d', errors='coerce')\n",
    "cleaned_df['delivery date'] = pd.to_datetime(cleaned_df['delivery date'], format='%Y%m%d', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOmVv94qu_On"
   },
   "outputs": [],
   "source": [
    "\n",
    "cleaned_df.sort_values(by='item_date', ascending=False, inplace=True)\n",
    "cleaned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbrv4OvhrU6P"
   },
   "outputs": [],
   "source": [
    "# Some rubbish values are present in ‘Material_Reference’ which starts with ‘00000’ value which should be converted into null\n",
    "cleaned_df['material_ref'] = cleaned_df['material_ref'].apply(lambda x: None if str(x).startswith('00000') else x)\n",
    "# Replace commas with '@' in the material_ref column\n",
    "cleaned_df['material_ref'] = cleaned_df['material_ref'].str.replace(',', '@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ota2nvmnj13U"
   },
   "outputs": [],
   "source": [
    "cleaned_df['customer'] = cleaned_df['customer'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McOVPnsdqrYr"
   },
   "outputs": [],
   "source": [
    "# Checking for consistency in categorization\n",
    "cleaned_df['country'] = cleaned_df['country'].astype('category')\n",
    "cleaned_df['status'] = cleaned_df['status'].astype('category')\n",
    "cleaned_df['item type'] = cleaned_df['item type'].astype('category')\n",
    "cleaned_df['application'] = cleaned_df['application'].astype('category')\n",
    "cleaned_df['product_ref'] = cleaned_df['product_ref'].astype('category')\n",
    "cleaned_df['material_ref'] = cleaned_df['material_ref'].astype('category')\n",
    "cleaned_df['customer'] = cleaned_df['customer'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_Wb25JLo1Br"
   },
   "outputs": [],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XANiy17dr8ON"
   },
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "\n",
    "cleaned_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (replace with a default value or fill using a specific strategy)\n",
    "cleaned_df['quantity tons'].fillna(0, inplace=True) \n",
    "cleaned_df['thickness'].fillna(0, inplace=True) \n",
    "cleaned_df['width'].fillna(0, inplace=True) \n",
    "cleaned_df['selling_price'].fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = cleaned_df[cleaned_df.select_dtypes(include=['number']).columns].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display skewness for each numerical column\n",
    "print(\"Skewness for each numerical column:\")\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = (cleaned_df[cleaned_df.select_dtypes(include=['number']).columns] - cleaned_df[cleaned_df.select_dtypes(include=['number']).columns].mean()).abs() > 3 * cleaned_df[cleaned_df.select_dtypes(include=['number']).columns].std()  # Define your outlier detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Assume 'Selling_Price' is the target variable for regression\n",
    "# Assume 'STATUS' is the target variable for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "regression_features = cleaned_df.drop(['selling_price', 'status','id','item_date','delivery date','material_ref', 'customer','item type','product_ref'], axis=1)\n",
    "regression_target = cleaned_df['selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "classification_features = cleaned_df.drop(['selling_price','id','item_date','delivery date','material_ref', 'customer','item type','product_ref'], axis=1)\n",
    "classification_target = cleaned_df['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train-test split\n",
    "regression_X_train, regression_X_test, regression_y_train, regression_y_test = train_test_split(\n",
    "    regression_features, regression_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "classification_X_train, classification_X_test, classification_y_train, classification_y_test = train_test_split(\n",
    "    classification_features, classification_target, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data normalization and feature scaling\n",
    "scaler = StandardScaler()\n",
    "regression_X_train_scaled = scaler.fit_transform(regression_X_train)\n",
    "regression_X_test_scaled = scaler.transform(regression_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Regression model\n",
    "regression_model = RandomForestRegressor()\n",
    "regression_model.fit(regression_X_train_scaled, regression_y_train)\n",
    "regression_predictions = regression_model.predict(regression_X_test_scaled)\n",
    "regression_rmse = np.sqrt(mean_squared_error(regression_y_test, regression_predictions))\n",
    "\n",
    "# Classification model\n",
    "classification_model = RandomForestClassifier()\n",
    "classification_model.fit(classification_X_train, classification_y_train)\n",
    "classification_predictions = classification_model.predict(classification_X_test)\n",
    "classification_accuracy = accuracy_score(classification_y_test, classification_predictions)\n",
    "\n",
    "# Streamlit App\n",
    "st.title(\"Copper Industry ML Application\")\n",
    "\n",
    "# Sidebar for user input\n",
    "st.sidebar.title(\"Insert Column Values\")\n",
    "user_input = {}\n",
    "for column in df.columns:\n",
    "    user_input[column] = st.sidebar.text_input(f\"Enter {column}\", df[column].iloc[0])\n",
    "\n",
    "# Predictions\n",
    "regression_input = pd.DataFrame([user_input])\n",
    "classification_input = pd.DataFrame([user_input.drop(['Selling_Price'])])\n",
    "\n",
    "# Scaling and prediction for regression\n",
    "regression_input_scaled = scaler.transform(regression_input)\n",
    "predicted_selling_price = regression_model.predict(regression_input_scaled)\n",
    "\n",
    "# Prediction for classification\n",
    "predicted_status = classification_model.predict(classification_input)[0]\n",
    "\n",
    "# Display predictions\n",
    "st.header(\"Regression Prediction (Selling Price)\")\n",
    "st.write(f\"The predicted Selling Price is: {predicted_selling_price[0]}\")\n",
    "\n",
    "st.header(\"Classification Prediction (Status)\")\n",
    "st.write(f\"The predicted Status is: {predicted_status}\")\n",
    "\n",
    "# Display model evaluation metrics\n",
    "st.header(\"Model Evaluation Metrics\")\n",
    "st.write(f\"Regression RMSE: {regression_rmse}\")\n",
    "st.write(f\"Classification Accuracy: {classification_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
